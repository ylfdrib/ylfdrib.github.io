#信息熵
entropy, 熵是随机变量不确定性的度量

$$
H(X)=-\sum_{i=1}^{n}p(x_i)logp(x_i)
$$
在信息论中，将一串输入字符按二进制编码，信息熵是理论上的最小平均编码长度，或者最优编码每个字符需要的比特数。

比如：字符串ABBC如果以二进制编码，最小平均编码长度就是
$$
length = H(X) \\
  = -p(A)logp(A) - p(B)logp(B) - p(C)logp(C) \\
  =-1/4log(1/4) - 1/2log(1/2) - 1/4log(1/4) \\
  =1.5
$$
huffman编码：encode(A)=00, encode(B)=1, encode(C)=01，则encode(ABBC)=001101，编码总长度为6，字符串长度为4，平均编码长度为6/4=1.5


#条件熵
在x取值一定的情况下随机变量y不确定性的度量
$$
H(Y|X) = \sum_{x \in X} p(x)H(Y|X=x)
$$

#信息增益
information gain, 信息增益就是熵和特征条件熵的差，就是以某特征A划分数据集D前后的熵(不确定性)的差值
 
    信息增益 =  entroy(前) -  entroy(后)
$$
 g(D,A)= H(D) - H(D|A)
$$


**缺点**：信息增益偏向取值较多的特征
**原因**：当特征的取值较多时，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，由于划分前的熵是一定的，因此信息增益更大，因此信息增益比较 偏向取值较多的特征。 缺点：信息增益偏向取值较多的特征
 

#信息增益比
information gain rate

    信息增益比 = 惩罚参数 * 信息增益
$$
 g_R(D,A) = \frac {g(D,A)}{H_A(D)} 
$$
**注意**：其中的$$H_A(D)$$，对于样本集合$$D$$，将当前特征A作为随机变量（取值是特征A的各个特征值），求得的经验熵。
（之前是把集合类别作为随机变量，现在把某个特征作为随机变量，按照此特征的特征取值对集合D进行划分，计算熵HA(D)）
$$
H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}
$$
**缺点**：信息增益比偏向取值较少的特征   
**原因**：当特征取值较少时$$H_A(D)$$的值较小，因此其倒数较大，因而信息增益比较大。因而偏向取值较少的特征。
**使用信息增益比**：基于以上缺点，并不是直接选择信息增益率最大的特征，而是先在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征。


#基尼指数
基尼指数(基尼不纯度)：表示在样本集合中一个随机选中的样本被分错的概率。
**注意**：基尼指数越小，被分错的概率越小，集合纯度越高。

     基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率
$$
Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}{p_k}^2
$$     

**说明**:

1. pk表示选中的样本属于k类别的概率，则这个样本被分错的概率是(1-pk)
2. 样本集合中有K个类别，一个随机选中的样本可以属于这k个类别中的任意一个，因而对类别就加和
3. 当为二分类是
$$Gini(p) = 2p(1-p) 
  (令a=p,b=1-p; 则a+b=1, Gini(p)=2ab = 1 - (a^2 + b^2))
$$

**样本集合D的Gini指数** ： 假设集合中有K个类别，则：
$$
Gini(D) = 1-\sum_{k=1}^{K}(\frac{|C_k|}{|D|})
$$

CART是个二叉树，基于特征A划分样本集合D之后的基尼指数：
$$
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)
$$




参考[决策树](https://www.cnblogs.com/muzixi/p/6566803.html)


#相对熵
relative entropy, 也被称为KL散度(Kullback-Leibler divergence, KLD),或者信息散度(information divergence),是两个概率分布间差异的非对称度量。在信息论中，相对熵等价于两个概率分布信息熵的差值。
$$
KL(P\|Q)=\sum_{}P(x)log\frac{P(x)}{Q(x)}
$$
在信息理论中，相对熵是用来度量使用基于Q的编码来编码来自P的样本平均所需的额外的比特个数。典型情况下，P表示数据的真实分布，Q表示数据的理论分布，模型分布，或P的近似分布。

**性质**：
1. 非负性，$$KL(P \| Q) \geq 0$$，且在$$P \equiv Q$$时取0.
2. 不对称性，$$KL(p, q) \neq KL(q, p)$$

#交叉熵
cross entropy, 

$$
H(P,Q)=-\sum_{}P(x)logQ(x) \\
KL(P\|Q) = H(P,Q) - H(P)
$$

在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即$$KL_D(y||\hat{y} )$$，由于KL散度中的后一部分-H(y)不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做loss，评估模型。

在二分类(n=2)问题上，单个样本的loss
$$
loss=-\sum_{i=1}^{n} y_{i} \log (\hat{y}_{i}) \\
loss=ylog(\hat{y}) + (1-y)log(1-\hat{y})
$$
全部样本的平均loss
$$
loss=-\frac{1}{m} \sum_{j=1}^{m} \sum_{i=1}^{n} y_{j i} \log \left(\hat{y}_{j i}\right) \\
loss=-\frac{1}{m}\sum_{j=1}^{m}(ylog(\hat{y}) + (1-y)log(1-\hat{y})
)
$$
其中，m为样本量
求交叉熵就和求最大似然估计（假设样本服从伯努利分布）一致了。






    